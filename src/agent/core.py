"""Core Meta AI Agent implementation."""

import asyncio
import json
from datetime import datetime, time, timedelta
from pathlib import Path
from typing import Any

from loguru import logger

from src.agent.action import ActionExecutor, ExecutionSummary
from src.agent.claude_client import ClaudeClient
from src.agent.decision import AgentAction, AgentDecision, ActionType, AutonomyLevel
from src.agent.long_term_memory import LongTermMemory, ConfidenceLevel
from src.agent.memory import AgentMemory, SignalOutcome
from src.agent.perception import AgentContext, PerceptionModule
from src.agent.schedule import Scheduler, TaskFrequency, DEFAULT_TASKS
from src.features.registry import FeatureRegistry
from src.utils.timezone import now_jst


class MetaAgent:
    """
    Autonomous Meta AI Agent for trading system oversight.

    Responsibilities:
    - Monitor trading system health and performance
    - Verify signal predictions against actual outcomes
    - Adjust parameters and features based on analysis
    - Conduct daily reviews and generate reports
    - Handle emergency situations
    """

    def __init__(
        self,
        api_base_url: str = "http://localhost:8088",
        anthropic_api_key: str | None = None,
        telegram_token: str | None = None,
        telegram_chat_id: str | None = None,
        db_path: str = "data/agent_memory.db",
        feature_registry_path: str = "data/feature_registry.json",
        long_term_memory_dir: str = "data/memory",
        check_interval: int = 60,  # seconds
    ) -> None:
        """
        Initialize Meta AI Agent.

        Args:
            api_base_url: Trading bot API URL
            anthropic_api_key: Anthropic API key for Claude
            telegram_token: Telegram bot token
            telegram_chat_id: Telegram chat ID
            db_path: Path to agent memory database
            feature_registry_path: Path to feature registry config
            long_term_memory_dir: Path to long-term memory directory
            check_interval: Interval between state checks (seconds)
        """
        self.api_base_url = api_base_url
        self.check_interval = check_interval

        # Initialize components
        self.claude = ClaudeClient(api_key=anthropic_api_key)
        self.memory = AgentMemory(db_path=db_path)
        self.long_term_memory = LongTermMemory(memory_dir=long_term_memory_dir)
        self.perception = PerceptionModule(api_base_url=api_base_url)
        self.feature_registry = FeatureRegistry(config_path=feature_registry_path)
        self.executor = ActionExecutor(
            api_base_url=api_base_url,
            telegram_token=telegram_token,
            telegram_chat_id=telegram_chat_id,
            memory=self.memory,
            feature_registry=self.feature_registry,
        )
        self.scheduler = Scheduler()

        # State tracking
        self._running = False
        self._last_decision_time: datetime | None = None
        self._last_context: AgentContext | None = None
        self._consecutive_errors = 0
        self._max_consecutive_errors = 5

        # Setup scheduled tasks
        self._setup_scheduled_tasks()

        logger.info("Meta AI Agent initialized")

    def _setup_scheduled_tasks(self) -> None:
        """Setup default scheduled tasks."""

        # Market check (every minute)
        self.scheduler.add_task(
            name="market_check",
            task_func=self._task_market_check,
            frequency=TaskFrequency.INTERVAL,
            interval=timedelta(minutes=1),
        )

        # Signal verification (every 15 minutes)
        self.scheduler.add_task(
            name="signal_verification",
            task_func=self._task_signal_verification,
            frequency=TaskFrequency.INTERVAL,
            interval=timedelta(minutes=15),
        )

        # Performance snapshot (every hour)
        self.scheduler.add_task(
            name="performance_snapshot",
            task_func=self._task_performance_snapshot,
            frequency=TaskFrequency.INTERVAL,
            interval=timedelta(hours=1),
        )

        # Daily review at 21:00 JST
        self.scheduler.add_task(
            name="daily_review",
            task_func=self._task_daily_review,
            frequency=TaskFrequency.DAILY,
            run_time=time(21, 0),
        )

        # Morning preparation at 08:00 JST
        self.scheduler.add_task(
            name="morning_prep",
            task_func=self._task_morning_prep,
            frequency=TaskFrequency.DAILY,
            run_time=time(8, 0),
        )

        # Weekly summary on Sunday 20:00 JST
        self.scheduler.add_task(
            name="weekly_summary",
            task_func=self._task_weekly_summary,
            frequency=TaskFrequency.WEEKLY,
            run_time=time(20, 0),
            run_day=6,  # Sunday
        )

        # Feature optimization on Saturday 19:00 JST (weekly)
        self.scheduler.add_task(
            name="feature_optimization",
            task_func=self._task_feature_optimization,
            frequency=TaskFrequency.WEEKLY,
            run_time=time(19, 0),
            run_day=5,  # Saturday
        )

        # Database maintenance on Sunday 03:00 JST (weekly, low traffic time)
        self.scheduler.add_task(
            name="database_maintenance",
            task_func=self._task_database_maintenance,
            frequency=TaskFrequency.WEEKLY,
            run_time=time(3, 0),
            run_day=6,  # Sunday
        )

        # Memory validation on Sunday 04:00 JST (after DB maintenance)
        self.scheduler.add_task(
            name="memory_validation",
            task_func=self._task_memory_validation,
            frequency=TaskFrequency.WEEKLY,
            run_time=time(4, 0),
            run_day=6,  # Sunday
        )

        logger.info("Scheduled tasks configured")

    async def run(self) -> None:
        """
        Main agent loop.
        Runs continuously, checking state and executing tasks.
        """
        self._running = True
        logger.info("Meta AI Agent starting main loop")

        # Send startup notification
        await self.executor._send_telegram(
            "ü§ñ Meta AI Agent Ëµ∑Âãï\n"
            f"Áõ£Ë¶ñÈñìÈöî: {self.check_interval}Áßí\n"
            f"API: {self.api_base_url}"
        )

        try:
            while self._running:
                try:
                    # 0. Check for manual triggers (from API/Telegram)
                    await self._check_triggers()

                    # 1. Gather current context
                    context = await self.perception.get_context()
                    self._last_context = context

                    # 2. Update status file for API/Telegram
                    await self._update_status_file()

                    # 3. Check if attention is needed
                    if context.needs_attention():
                        logger.info("Context requires attention, running decision cycle")
                        await self._decision_cycle(context)

                    # 4. Run scheduled tasks
                    await self.scheduler.check_and_run()

                    # Reset error counter on success
                    self._consecutive_errors = 0

                except Exception as e:
                    self._consecutive_errors += 1
                    logger.error(f"Error in main loop: {e}")

                    if self._consecutive_errors >= self._max_consecutive_errors:
                        logger.critical(
                            f"Too many consecutive errors ({self._consecutive_errors}), "
                            "sending alert and pausing"
                        )
                        await self.executor._send_telegram(
                            f"üö® Meta Agent „Ç®„É©„ÉºÂ§öÁô∫\n"
                            f"ÈÄ£Á∂ö„Ç®„É©„ÉºÊï∞: {self._consecutive_errors}\n"
                            f"ÊúÄÊñ∞„Ç®„É©„Éº: {e}\n\n"
                            "Á¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
                        )
                        # Wait longer before retrying
                        await asyncio.sleep(300)  # 5 minutes
                        self._consecutive_errors = 0

                # Wait before next cycle
                await asyncio.sleep(self.check_interval)

        except asyncio.CancelledError:
            logger.info("Agent main loop cancelled")
        finally:
            self._running = False
            await self._cleanup()

    async def stop(self) -> None:
        """Stop the agent gracefully."""
        logger.info("Stopping Meta AI Agent...")
        self._running = False
        # Wait a moment for current iteration to finish
        await asyncio.sleep(1)
        await self._cleanup()

    async def _cleanup(self) -> None:
        """Cleanup resources and save state."""
        logger.info("Cleaning up agent resources...")

        # Close HTTP sessions
        await self.perception.close()
        await self.executor.close()

        # Save feature registry state
        self.feature_registry.save_config()
        logger.info("Feature registry saved")

        # Close memory database properly
        self.memory.close()

        logger.info("Agent resources cleaned up")

    async def _decision_cycle(self, context: AgentContext) -> None:
        """
        Run a full decision cycle.

        1. Build prompt from context
        2. Ask Claude for decision (with short-term and long-term memory)
        3. Execute decided actions
        4. Record decision and results
        """
        # Get short-term memory (recent decisions)
        memory_summary = self.memory.get_decision_history_summary(limit=10)

        # Get long-term memory (learned insights and rules)
        long_term_context = self.long_term_memory.get_context_for_prompt()

        # Ask Claude for decision
        decision = await self.claude.analyze_and_decide(
            context_prompt=context.to_prompt(),
            memory_summary=memory_summary,
            long_term_memory=long_term_context,
        )

        # Log decision
        logger.info(
            f"Decision made: {len(decision.actions)} actions, "
            f"confidence={decision.confidence:.2f}"
        )

        if not decision.actions:
            logger.debug("No actions decided")
            return

        # Execute actions based on autonomy level
        results = await self.executor.execute_actions(decision.actions)

        # Record decision and results
        self.memory.record_decision(
            context_summary=self._summarize_context(context),
            decision=decision,
            results=[r.to_dict() for r in results.results],
            success=results.overall_success,
        )

        self._last_decision_time = now_jst()

    def _summarize_context(self, context: AgentContext) -> str:
        """Create a brief summary of context for storage."""
        parts = []

        if context.market:
            parts.append(f"BTC: ¬•{context.market.current_price:,.0f}")

        if context.performance:
            parts.append(f"Êó•Ê¨°PnL: ¬•{context.performance.daily_pnl:+,.0f}")

        if context.system_health:
            parts.append(f"„Ç∑„Çπ„ÉÜ„É†: {context.system_health.status}")

        parts.append(f"„Ç∑„Ç∞„Éä„É´: {len(context.recent_signals)}‰ª∂")
        parts.append(f"ÂèñÂºï: {len(context.recent_trades)}‰ª∂")

        return " | ".join(parts)

    # ==================== Scheduled Tasks ====================

    async def _task_market_check(self) -> None:
        """Quick market state check for anomalies."""
        context = await self.perception.get_context()

        # Check for rapid price movement
        if context.market:
            if abs(context.market.price_change_1h) >= 0.03:  # 3% in 1 hour
                logger.warning(
                    f"Rapid price movement detected: {context.market.price_change_1h:+.2%}"
                )
                # This will trigger attention in the next cycle

    async def _task_signal_verification(self) -> None:
        """Verify recent signals against actual outcomes."""
        logger.info("Running signal verification")

        # Get signals from the last hour that need verification
        signals = await self.perception.get_recent_signals(hours=2)

        verified_count = 0
        for signal in signals:
            # Skip if already verified or too recent
            if signal.outcome is not None:
                continue

            # Check if enough time has passed (1 hour after signal)
            time_since_signal = now_jst() - signal.timestamp
            if time_since_signal < timedelta(hours=1):
                continue

            # Get actual price movement
            actual_move = await self.perception.calculate_price_move(
                symbol="BTC_JPY",
                start_time=signal.timestamp,
                end_time=signal.timestamp + timedelta(hours=1),
            )

            if actual_move is None:
                continue

            # Determine if prediction was correct
            # LONG prediction is correct if price went up by threshold (0.3%)
            # SHORT prediction is correct if price went down by threshold
            threshold = 0.003  # 0.3%
            was_correct = False

            if signal.direction == "LONG":
                was_correct = actual_move >= threshold
            else:  # SHORT
                was_correct = actual_move <= -threshold

            # Record outcome
            outcome = SignalOutcome(
                signal_id=signal.id,
                timestamp=signal.timestamp,
                direction=signal.direction,
                confidence=signal.confidence,
                price_at_signal=signal.price,
                price_after_1h=signal.price * (1 + actual_move),
                actual_move=actual_move,
                was_correct=was_correct,
                analysis="",
                feature_insights=[],
                suggestions=[],
            )

            self.memory.record_signal_outcome(outcome)
            verified_count += 1

        if verified_count > 0:
            logger.info(f"Verified {verified_count} signals")

            # Get stats and report if accuracy is concerning
            stats = self.memory.get_signal_accuracy_stats(days=1)
            if stats["evaluated"] >= 5 and stats["accuracy"] < 0.4:
                await self.executor._send_telegram(
                    f"‚ö†Ô∏è „Ç∑„Ç∞„Éä„É´Á≤æÂ∫¶‰Ωé‰∏ã\n"
                    f"ÈÅéÂéª24ÊôÇÈñì„ÅÆÊ≠£Ëß£Áéá: {stats['accuracy']:.1%}\n"
                    f"Ê§úË®ºÊï∞: {stats['evaluated']}\n"
                    f"LONG: {stats['long_accuracy']:.1%}\n"
                    f"SHORT: {stats['short_accuracy']:.1%}"
                )

    async def _task_performance_snapshot(self) -> None:
        """Take a performance snapshot."""
        context = await self.perception.get_context()

        if context.performance:
            logger.info(
                f"Performance snapshot: "
                f"capital=¬•{context.performance.capital:,.0f}, "
                f"daily_pnl=¬•{context.performance.daily_pnl:+,.0f}, "
                f"win_rate={context.performance.win_rate:.1%}"
            )

    async def _task_daily_review(self) -> None:
        """Run daily review (reflection meeting)."""
        logger.info("Starting daily review")

        # Gather data for review
        signals = await self.perception.get_recent_signals(hours=24)
        trades = await self.perception.get_recent_trades(hours=24)
        performance = await self.perception.get_performance_metrics()

        # Get signal accuracy stats
        signal_stats = self.memory.get_signal_accuracy_stats(days=1)

        # Run intervention analysis (missed opportunities, stop-loss timing, etc.)
        intervention_results = await self._analyze_interventions(trades, signals)

        # Build data for Claude analysis
        signals_data = [s.to_dict() for s in signals]
        trades_data = [t.to_dict() for t in trades]
        performance_data = performance.to_dict() if performance else {}

        # Get market summary
        market = await self.perception.get_market_state()
        market_summary = f"BTC: ¬•{market.current_price:,.0f}" if market else "Â∏ÇÂ†¥„Éá„Éº„Çø„Å™„Åó"

        # Generate review with Claude (including intervention analysis)
        review_report = await self.claude.generate_daily_review(
            signals_data=signals_data,
            trades_data=trades_data,
            performance_data=performance_data,
            market_summary=market_summary,
            intervention_summary=intervention_results.get("summary", ""),
        )

        # Build intervention stats text
        intervention_text = ""
        if intervention_results["analyses"]:
            intervention_text = f"\n\nüìä ‰ªãÂÖ•ÂàÜÊûê: {len(intervention_results['analyses'])}‰ª∂Ê§úÂá∫"
            if intervention_results.get("obvious_count", 0) > 0:
                intervention_text += f"\n  ‚ö†Ô∏è ÊòéÁôΩ„Å™Ë¶ãÈÄÉ„Åó: {intervention_results['obvious_count']}‰ª∂"

        # Build long-term memory stats text
        ltm_stats = self.long_term_memory.get_stats()
        ltm_text = f"\n\nüß† Èï∑ÊúüË®òÊÜ∂:\n"
        ltm_text += f"- ÊúâÂäπ„Å™Ê¥ûÂØü: {ltm_stats['insights']['active']}‰ª∂"
        if ltm_stats['insights']['under_review'] > 0:
            ltm_text += f" („É¨„Éì„É•„Éº‰∏≠: {ltm_stats['insights']['under_review']}‰ª∂)"
        ltm_text += f"\n- ÊúâÂäπ„Å™„É´„Éº„É´: {ltm_stats['rules']['active']}‰ª∂"
        if ltm_stats['rules']['under_review'] > 0:
            ltm_text += f" („É¨„Éì„É•„Éº‰∏≠: {ltm_stats['rules']['under_review']}‰ª∂)"

        # Send report
        await self.executor._send_telegram(
            f"üìã Êó•Ê¨°„É¨„Éì„É•„Éº ({now_jst().strftime('%Y-%m-%d')})\n\n"
            f"„Ç∑„Ç∞„Éä„É´Áµ±Ë®à:\n"
            f"- Ê§úË®ºÊï∞: {signal_stats['evaluated']}\n"
            f"- Ê≠£Ëß£Áéá: {signal_stats['accuracy']:.1%}\n"
            f"- LONG: {signal_stats['long_accuracy']:.1%}\n"
            f"- SHORT: {signal_stats['short_accuracy']:.1%}"
            f"{intervention_text}"
            f"{ltm_text}\n\n"
            f"{review_report[:2600]}"  # Telegram limit (adjusted for additional text)
        )

        # Extract insights from review and save to long-term memory
        await self._extract_and_save_insights(
            review_report=review_report,
            performance_data=performance_data,
            signal_stats=signal_stats,
        )

        logger.info("Daily review completed")

    async def _extract_and_save_insights(
        self,
        review_report: str,
        performance_data: dict,
        signal_stats: dict,
    ) -> None:
        """
        Extract insights from daily review and save to long-term memory.

        Args:
            review_report: The daily review report text
            performance_data: Performance metrics
            signal_stats: Signal accuracy statistics
        """
        logger.info("Extracting insights from daily review")

        try:
            # Extract insights using Claude
            extracted = await self.claude.extract_insights_from_review(
                daily_review=review_report,
                performance_data=performance_data,
                signal_accuracy=signal_stats,
            )

            insights_added = 0
            rules_added = 0
            events_added = 0

            # Save insights
            for insight_data in extracted.get("insights", []):
                try:
                    confidence = ConfidenceLevel(insight_data.get("confidence", "low"))
                    self.long_term_memory.add_insight(
                        category=insight_data.get("category", "„Åù„ÅÆ‰ªñ"),
                        title=insight_data.get("title", ""),
                        content=insight_data.get("content", ""),
                        evidence=insight_data.get("evidence", []),
                        conditions=insight_data.get("conditions", []),
                        confidence=confidence,
                    )
                    insights_added += 1
                except Exception as e:
                    logger.warning(f"Failed to save insight: {e}")

            # Save rules
            for rule_data in extracted.get("rules", []):
                try:
                    confidence = ConfidenceLevel(rule_data.get("confidence", "low"))
                    self.long_term_memory.add_rule(
                        name=rule_data.get("name", ""),
                        rule_type=rule_data.get("type", "conditional"),
                        content=rule_data.get("content", ""),
                        origin=rule_data.get("origin", "Êó•Ê¨°„É¨„Éì„É•„Éº„Åã„ÇâÊäΩÂá∫"),
                        confidence=confidence,
                    )
                    rules_added += 1
                except Exception as e:
                    logger.warning(f"Failed to save rule: {e}")

            # Save events
            for event_data in extracted.get("events", []):
                try:
                    self.long_term_memory.add_event(
                        name=event_data.get("name", ""),
                        category=event_data.get("category", "other"),
                        severity=event_data.get("severity", "medium"),
                        impact=event_data.get("impact", ""),
                        situation=event_data.get("situation", ""),
                        response="Êó•Ê¨°„É¨„Éì„É•„Éº„ÅßË®òÈå≤",
                        result="",
                        lessons=event_data.get("lessons", []),
                    )
                    events_added += 1
                except Exception as e:
                    logger.warning(f"Failed to save event: {e}")

            if insights_added or rules_added or events_added:
                logger.info(
                    f"Long-term memory updated: "
                    f"{insights_added} insights, {rules_added} rules, {events_added} events"
                )

                # Notify about significant updates
                if insights_added + rules_added >= 2:
                    await self.executor._send_telegram(
                        f"üß† Èï∑ÊúüË®òÊÜ∂„ÇíÊõ¥Êñ∞„Åó„Åæ„Åó„Åü\n"
                        f"- Êñ∞„Åó„ÅÑÊ¥ûÂØü: {insights_added}‰ª∂\n"
                        f"- Êñ∞„Åó„ÅÑ„É´„Éº„É´: {rules_added}‰ª∂\n"
                        f"- Êñ∞„Åó„ÅÑ„Ç§„Éô„É≥„Éà: {events_added}‰ª∂"
                    )
            else:
                reason = extracted.get("no_new_insights_reason", "Áâπ„Å´Êñ∞„Åó„ÅÑÂ≠¶Áøí‰∫ãÈ†Ö„Å™„Åó")
                logger.info(f"No new insights extracted: {reason}")

        except Exception as e:
            logger.error(f"Failed to extract insights: {e}")

    async def _analyze_interventions(
        self,
        trades: list,
        signals: list,
    ) -> dict:
        """
        Analyze missed or delayed interventions.

        Evaluates:
        1. Stop-loss timing - Could we have exited earlier?
        2. Missed opportunities - Large moves without positions
        3. Threshold issues - Would lower thresholds have been profitable?

        Returns a dict with analysis results.
        """
        from src.agent.memory import InterventionAnalysis

        analyses = []
        obvious_count = 0

        # 1. Analyze stop-loss timing for losing trades
        for trade in trades:
            if not hasattr(trade, "pnl") or trade.pnl is None:
                continue

            # Only analyze closed losing trades
            if trade.pnl < 0 and hasattr(trade, "exit_price"):
                # Check if price moved significantly against position before SL hit
                analysis = await self._analyze_stop_loss_timing(trade)
                if analysis:
                    analyses.append(analysis)
                    self.memory.record_intervention_analysis(analysis)
                    if analysis.hindsight_difficulty == "obvious":
                        obvious_count += 1

        # 2. Detect large price moves without positions (missed opportunities)
        market_moves = await self._detect_significant_moves(hours=24)
        for move in market_moves:
            # Check if we had no position during this move
            had_position = self._had_position_during(move, trades)
            if not had_position and abs(move["change"]) >= 0.02:  # 2% move
                analysis = self._create_missed_opportunity_analysis(move, signals)
                if analysis:
                    analyses.append(analysis)
                    self.memory.record_intervention_analysis(analysis)
                    if analysis.hindsight_difficulty == "obvious":
                        obvious_count += 1

        # Build summary for Claude
        summary_parts = []
        if analyses:
            summary_parts.append(f"Ê§úÂá∫„Åï„Çå„Åü‰ªãÂÖ•ÂàÜÊûê: {len(analyses)}‰ª∂")

            by_type = {}
            for a in analyses:
                by_type[a.analysis_type] = by_type.get(a.analysis_type, 0) + 1

            for t, count in by_type.items():
                type_label = {
                    "stop_loss_timing": "ÊêçÂàá„Çä„Çø„Ç§„Éü„É≥„Ç∞",
                    "missed_opportunity": "Ê©ü‰ºöÊêçÂ§±",
                    "threshold_too_strict": "ÈñæÂÄ§ÂïèÈ°å",
                }.get(t, t)
                summary_parts.append(f"  - {type_label}: {count}‰ª∂")

            if obvious_count > 0:
                summary_parts.append(f"\n‚ÄªÊòéÁôΩ„Å™Ë¶ãÈÄÉ„Åó: {obvious_count}‰ª∂ (Ë¶ÅÊîπÂñÑ)")

        return {
            "analyses": analyses,
            "obvious_count": obvious_count,
            "summary": "\n".join(summary_parts) if summary_parts else "Áâπ„Å´ÂïèÈ°å„Å™„Åó",
        }

    async def _analyze_stop_loss_timing(self, trade) -> "InterventionAnalysis | None":
        """Analyze if stop-loss could have been triggered earlier."""
        from src.agent.memory import InterventionAnalysis

        # Get price history during trade
        try:
            price_history = await self.perception.get_price_history(
                symbol="BTC_JPY",
                start_time=trade.entry_time,
                end_time=trade.exit_time if hasattr(trade, "exit_time") else None,
            )
        except Exception:
            return None

        if not price_history:
            return None

        # Find if there was an earlier opportunity to exit with less loss
        entry_price = trade.entry_price
        exit_price = trade.exit_price if hasattr(trade, "exit_price") else entry_price
        actual_loss = trade.pnl

        # For LONG: find highest price after entry
        # For SHORT: find lowest price after entry
        best_exit_price = None
        best_exit_time = None

        for point in price_history:
            price = point.get("close", point.get("price"))
            if trade.side == "BUY":  # LONG
                if best_exit_price is None or price > best_exit_price:
                    best_exit_price = price
                    best_exit_time = point.get("timestamp")
            else:  # SHORT
                if best_exit_price is None or price < best_exit_price:
                    best_exit_price = price
                    best_exit_time = point.get("timestamp")

        if best_exit_price is None:
            return None

        # Calculate potential better outcome
        if trade.side == "BUY":
            potential_pnl = (best_exit_price - entry_price) * trade.size
        else:
            potential_pnl = (entry_price - best_exit_price) * trade.size

        # Only report if significant improvement was possible
        improvement = potential_pnl - actual_loss
        if improvement < abs(actual_loss) * 0.3:  # Less than 30% improvement
            return None

        # Determine hindsight difficulty
        # If the better exit was clearly signaled (e.g., RSI extreme), it's "obvious"
        # Otherwise, it's "moderate" or "difficult"
        hindsight_difficulty = "moderate"  # Default

        # If price reversed sharply (>1% in 15 min), it was probably predictable
        if abs(best_exit_price - exit_price) / exit_price > 0.01:
            hindsight_difficulty = "obvious" if improvement > abs(actual_loss) else "moderate"

        return InterventionAnalysis(
            id=None,
            timestamp=trade.exit_time if hasattr(trade, "exit_time") else now_jst(),
            analysis_type="stop_loss_timing",
            trade_id=trade.id if hasattr(trade, "id") else None,
            price_at_event=exit_price,
            optimal_action=f"¬•{best_exit_price:,.0f}„ÅßÊ±∫Ê∏à",
            actual_action=f"¬•{exit_price:,.0f}„ÅßÊêçÂàá„Çä",
            potential_impact=improvement,
            hindsight_difficulty=hindsight_difficulty,
            contributing_factors=[
                f"ÊúÄËâØÊ±∫Ê∏à‰æ°Ê†º: ¬•{best_exit_price:,.0f}",
                f"ÂÆüÈöõ„ÅÆÊ±∫Ê∏à: ¬•{exit_price:,.0f}",
                f"ÊîπÂñÑÂèØËÉΩÈ°ç: ¬•{improvement:,.0f}",
            ],
            recommendation="ATRÂÄçÁéá„ÅÆË¶ãÁõ¥„Åó„Åæ„Åü„ÅØÊôÇÈñì„Éô„Éº„Çπ„ÅÆÊêçÂàá„Çä„É´„Éº„É´Ê§úË®é",
            evaluated_by_llm=False,
        )

    async def _detect_significant_moves(self, hours: int = 24) -> list[dict]:
        """Detect significant price movements in the past N hours."""
        try:
            # Get hourly price data
            price_history = await self.perception.get_price_history(
                symbol="BTC_JPY",
                start_time=now_jst() - timedelta(hours=hours),
                interval="1h",
            )
        except Exception:
            return []

        if not price_history or len(price_history) < 2:
            return []

        moves = []
        for i in range(1, len(price_history)):
            prev = price_history[i - 1]
            curr = price_history[i]

            prev_price = prev.get("close", prev.get("price", 0))
            curr_price = curr.get("close", curr.get("price", 0))

            if prev_price == 0:
                continue

            change = (curr_price - prev_price) / prev_price

            if abs(change) >= 0.015:  # 1.5% or more
                moves.append({
                    "start_time": prev.get("timestamp"),
                    "end_time": curr.get("timestamp"),
                    "start_price": prev_price,
                    "end_price": curr_price,
                    "change": change,
                    "direction": "up" if change > 0 else "down",
                })

        return moves

    def _had_position_during(self, move: dict, trades: list) -> bool:
        """Check if we had a position during a price move."""
        move_start = move.get("start_time")
        move_end = move.get("end_time")

        if not move_start or not move_end:
            return False

        # Convert to datetime if string
        if isinstance(move_start, str):
            move_start = datetime.fromisoformat(move_start.replace("Z", "+00:00"))
        if isinstance(move_end, str):
            move_end = datetime.fromisoformat(move_end.replace("Z", "+00:00"))

        for trade in trades:
            trade_start = trade.entry_time if hasattr(trade, "entry_time") else None
            trade_end = trade.exit_time if hasattr(trade, "exit_time") else now_jst()

            if trade_start is None:
                continue

            # Check for overlap
            if trade_start <= move_end and trade_end >= move_start:
                return True

        return False

    def _create_missed_opportunity_analysis(
        self,
        move: dict,
        signals: list,
    ) -> "InterventionAnalysis | None":
        """Create analysis for a missed trading opportunity."""
        from src.agent.memory import InterventionAnalysis

        change = move["change"]
        direction = move["direction"]

        # Check if there was a signal that could have caught this move
        matching_signals = []
        for signal in signals:
            signal_time = signal.timestamp if hasattr(signal, "timestamp") else None
            if signal_time is None:
                continue

            move_start = move.get("start_time")
            if isinstance(move_start, str):
                move_start = datetime.fromisoformat(move_start.replace("Z", "+00:00"))

            # Signal within 2 hours before the move
            if signal_time < move_start and (move_start - signal_time).total_seconds() < 7200:
                signal_direction = signal.direction if hasattr(signal, "direction") else None
                if signal_direction:
                    expected = "LONG" if direction == "up" else "SHORT"
                    if signal_direction == expected:
                        matching_signals.append(signal)

        # Determine hindsight difficulty
        if matching_signals:
            # We had a signal but didn't trade - check confidence
            max_conf = max(s.confidence for s in matching_signals if hasattr(s, "confidence"))
            if max_conf >= 0.6:
                hindsight_difficulty = "obvious"  # High confidence signal, should have traded
            else:
                hindsight_difficulty = "moderate"  # Low confidence, understandable miss
            contributing_factors = [
                f"ÈÅ©Âàá„Å™ÊñπÂêë„ÅÆ„Ç∑„Ç∞„Éä„É´„ÅÇ„Çä (‰ø°È†ºÂ∫¶: {max_conf:.1%})",
                f"‰æ°Ê†ºÂ§âÂãï: {change:+.2%}",
            ]
            recommendation = "‰ø°È†ºÂ∫¶ÈñæÂÄ§„Çí‰∏ã„Åí„Çã„Åì„Å®„ÇíÊ§úË®é"
        else:
            # No signal at all
            hindsight_difficulty = "difficult"  # No signal, hard to predict
            contributing_factors = [
                "„Ç∑„Ç∞„Éä„É´„Å™„Åó",
                f"‰æ°Ê†ºÂ§âÂãï: {change:+.2%}",
            ]
            recommendation = "ÁâπÂæ¥Èáè„ÅÆËøΩÂä†„Åæ„Åü„ÅØË¶ãÁõ¥„Åó„ÇíÊ§úË®é"

        # Calculate potential impact (rough estimate)
        move_pct = abs(change)
        # Assume 1% position size, so potential gain is move_pct * position
        potential_impact = move["start_price"] * 0.01 * move_pct  # Rough estimate

        return InterventionAnalysis(
            id=None,
            timestamp=datetime.fromisoformat(move["end_time"].replace("Z", "+00:00"))
            if isinstance(move["end_time"], str)
            else move["end_time"],
            analysis_type="missed_opportunity",
            trade_id=None,
            price_at_event=move["end_price"],
            optimal_action=f"{'LONG' if direction == 'up' else 'SHORT'}„Ç®„É≥„Éà„É™„Éº",
            actual_action="„Éé„Éº„Éù„Ç∏„Ç∑„Éß„É≥",
            potential_impact=potential_impact,
            hindsight_difficulty=hindsight_difficulty,
            contributing_factors=contributing_factors,
            recommendation=recommendation,
            evaluated_by_llm=False,
        )

    async def _task_morning_prep(self) -> None:
        """Morning preparation and status check."""
        logger.info("Running morning preparation")

        context = await self.perception.get_context()

        # Build status message
        lines = [f"üåÖ „Åä„ÅØ„Çà„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô ({now_jst().strftime('%Y-%m-%d %H:%M')})\n"]

        if context.system_health:
            status_emoji = {
                "healthy": "‚úÖ",
                "degraded": "‚ö†Ô∏è",
                "unhealthy": "üö®",
            }.get(context.system_health.status, "‚ùì")
            lines.append(f"„Ç∑„Çπ„ÉÜ„É†Áä∂ÊÖã: {status_emoji} {context.system_health.status}")

            if context.system_health.emergency_stop_active:
                lines.append("‚ö†Ô∏è Á∑äÊÄ•ÂÅúÊ≠¢‰∏≠")
            if context.system_health.long_stopped:
                lines.append("üî¥ LONGÂÅúÊ≠¢‰∏≠")
            if context.system_health.short_stopped:
                lines.append("üî¥ SHORTÂÅúÊ≠¢‰∏≠")

        if context.market:
            lines.append(f"\nBTC: ¬•{context.market.current_price:,.0f}")
            lines.append(f"24hÂ§âÂãï: {context.market.price_change_24h:+.2%}")

        if context.performance:
            lines.append(f"\nË≥áÊú¨: ¬•{context.performance.capital:,.0f}")
            lines.append(f"ÈÄ±ÈñìPnL: ¬•{context.performance.weekly_pnl:+,.0f}")
            lines.append(f"ÊúàÈñìPnL: ¬•{context.performance.monthly_pnl:+,.0f}")

        if context.open_positions:
            lines.append(f"\n„Ç™„Éº„Éó„É≥„Éù„Ç∏„Ç∑„Éß„É≥: {len(context.open_positions)}‰ª∂")

        # Get upcoming scheduled tasks
        upcoming = self.scheduler.get_upcoming_tasks(hours=24)
        if upcoming:
            lines.append("\nÊú¨Êó•„ÅÆ„Çπ„Ç±„Ç∏„É•„Éº„É´:")
            for task in upcoming[:5]:
                time_str = datetime.fromisoformat(task["next_run"]).strftime("%H:%M")
                lines.append(f"- {time_str}: {task['name']}")

        await self.executor._send_telegram("\n".join(lines))

    async def _task_weekly_summary(self) -> None:
        """Generate weekly summary report."""
        logger.info("Generating weekly summary")

        # Get weekly statistics
        signal_stats = self.memory.get_signal_accuracy_stats(days=7)
        decision_patterns = self.memory.get_decision_patterns()
        param_history = self.memory.get_param_history(days=7)

        # Get performance
        performance = await self.perception.get_performance_metrics()

        lines = [
            f"üìä ÈÄ±Ê¨°„Çµ„Éû„É™„Éº ({now_jst().strftime('%Y-%m-%d')})\n",
            "=== „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ ===",
        ]

        if performance:
            lines.extend([
                f"ÈÄ±ÈñìPnL: ¬•{performance.weekly_pnl:+,.0f}",
                f"ÂãùÁéá: {performance.win_rate:.1%}",
                f"ÂèñÂºïÊï∞: {performance.trades_count}Âõû",
            ])

        lines.extend([
            "\n=== „Ç∑„Ç∞„Éä„É´Á≤æÂ∫¶ ===",
            f"Ê§úË®ºÊï∞: {signal_stats['evaluated']}",
            f"Ê≠£Ëß£Áéá: {signal_stats['accuracy']:.1%}",
            f"LONG: {signal_stats['long_accuracy']:.1%}",
            f"SHORT: {signal_stats['short_accuracy']:.1%}",
        ])

        lines.extend([
            "\n=== „Ç®„Éº„Ç∏„Çß„É≥„ÉàÂà§Êñ≠ ===",
            f"Âà§Êñ≠Êï∞: {decision_patterns.get('total_evaluated', 0)}",
            f"ÊàêÂäüÁéá: {decision_patterns.get('success_rate', 0):.1%}",
        ])

        if param_history:
            lines.append(f"\n„Éë„É©„É°„Éº„ÇøÂ§âÊõ¥: {len(param_history)}‰ª∂")

        # Long-term memory statistics
        ltm_stats = self.long_term_memory.get_stats()
        lines.extend([
            "\n=== Èï∑ÊúüË®òÊÜ∂ ===",
            f"Ê¥ûÂØü: {ltm_stats['insights']['active']}‰ª∂ (ÊúâÂäπ)",
        ])
        if ltm_stats['insights']['under_review'] > 0:
            lines.append(f"  ‚îî „É¨„Éì„É•„Éº‰∏≠: {ltm_stats['insights']['under_review']}‰ª∂")
        if ltm_stats['insights']['deprecated'] > 0:
            lines.append(f"  ‚îî Ê∑òÊ±∞Ê∏à„Åø: {ltm_stats['insights']['deprecated']}‰ª∂")

        lines.append(f"„É´„Éº„É´: {ltm_stats['rules']['active']}‰ª∂ (ÊúâÂäπ)")
        if ltm_stats['rules']['under_review'] > 0:
            lines.append(f"  ‚îî „É¨„Éì„É•„Éº‰∏≠: {ltm_stats['rules']['under_review']}‰ª∂")
        if ltm_stats['rules']['deprecated'] > 0:
            lines.append(f"  ‚îî Ê∑òÊ±∞Ê∏à„Åø: {ltm_stats['rules']['deprecated']}‰ª∂")

        lines.append(f"„Ç§„Éô„É≥„ÉàÂ±•Ê≠¥: {ltm_stats['events']['total']}‰ª∂")

        # Show active high-confidence insights
        high_conf_insights = [
            i for i in self.long_term_memory.get_active_insights()
            if i.confidence.value == "high"
        ]
        if high_conf_insights:
            lines.append("\nüìå È´ò‰ø°È†ºÂ∫¶„ÅÆÊ¥ûÂØü:")
            for insight in high_conf_insights[:3]:
                lines.append(f"- [{insight.category}] {insight.title}")
                lines.append(f"  (Ê§úË®º{insight.verification_count}Âõû, ÊàêÂäüÁéá{insight.success_rate:.0%})")

        if decision_patterns.get("recommendations"):
            lines.append("\n=== ÊîπÂñÑÊèêÊ°à ===")
            for rec in decision_patterns["recommendations"][:3]:
                lines.append(f"- {rec}")

        await self.executor._send_telegram("\n".join(lines))
        logger.info("Weekly summary sent")

    async def _task_feature_optimization(self) -> None:
        """
        Weekly feature optimization task.
        Analyzes feature performance and recommends changes.
        """
        logger.info("Running feature optimization analysis")

        try:
            # Gather data for analysis
            feature_summary = self.feature_registry.get_summary()
            signal_stats = self.memory.get_signal_accuracy_stats(days=7)
            trades = await self.perception.get_recent_trades(hours=168)  # 7 days
            trades_data = [t.to_dict() for t in trades] if trades else []

            # Get model performance if available
            performance = await self.perception.get_performance_metrics()
            model_performance = performance.to_dict() if performance else {}

            # Ask Claude for feature optimization analysis
            optimization_result = await self.claude.analyze_feature_optimization(
                feature_registry_summary=feature_summary,
                model_performance=model_performance,
                signal_accuracy=signal_stats,
                recent_trades=trades_data,
            )

            # Process recommendations
            recommendations = optimization_result.get("feature_recommendations", [])
            executed_actions = []
            proposed_actions = []

            for rec in recommendations[:3]:  # Limit to 3 changes per week
                feature_name = rec.get("feature_name")
                action_type = rec.get("action")
                autonomy = rec.get("autonomy_level", "propose")
                reason = rec.get("reason", "")

                if action_type == "enable":
                    action = AgentAction(
                        action_type=ActionType.FEATURE_TOGGLE,
                        detail=f"ÁâπÂæ¥Èáè '{feature_name}' „ÇíÊúâÂäπÂåñ",
                        autonomy_level=AutonomyLevel(autonomy),
                        reasoning=reason,
                        parameters={"feature_name": feature_name, "enabled": True},
                    )
                elif action_type == "disable":
                    action = AgentAction(
                        action_type=ActionType.FEATURE_TOGGLE,
                        detail=f"ÁâπÂæ¥Èáè '{feature_name}' „ÇíÁÑ°ÂäπÂåñ",
                        autonomy_level=AutonomyLevel(autonomy),
                        reasoning=reason,
                        parameters={"feature_name": feature_name, "enabled": False},
                    )
                elif action_type == "update_importance":
                    action = AgentAction(
                        action_type=ActionType.FEATURE_IMPORTANCE_UPDATE,
                        detail=f"ÁâπÂæ¥Èáè '{feature_name}' „ÅÆÈáçË¶ÅÂ∫¶Êõ¥Êñ∞",
                        autonomy_level=AutonomyLevel.AUTO_EXECUTE,
                        reasoning=reason,
                        parameters={"importance": {feature_name: rec.get("importance_score", 0.5)}},
                    )
                else:
                    continue

                # Execute or propose based on autonomy level
                if autonomy in ["auto_execute", "auto_execute_report"]:
                    result = await self.executor.execute_actions([action])
                    executed_actions.append((feature_name, action_type, result.overall_success))
                else:
                    proposed_actions.append((feature_name, action_type, reason))

            # Build and send report
            report_lines = [
                f"üîß <b>ÈÄ±Ê¨°ÁâπÂæ¥ÈáèÊúÄÈÅ©Âåñ„É¨„Éù„Éº„Éà</b>",
                f"",
                f"<b>ÂàÜÊûêÁµêÊûú:</b>",
                f"{optimization_result.get('analysis', 'N/A')[:500]}",
                f"",
            ]

            if executed_actions:
                report_lines.append("<b>ÂÆüË°åÊ∏à„ÅøÂ§âÊõ¥:</b>")
                for name, action, success in executed_actions:
                    status = "‚úÖ" if success else "‚ùå"
                    report_lines.append(f"{status} {name}: {action}")

            if proposed_actions:
                report_lines.append("\n<b>ÊèêÊ°àÔºàÊâøË™çÂæÖ„Å°Ôºâ:</b>")
                for name, action, reason in proposed_actions:
                    report_lines.append(f"‚Ä¢ {name}: {action}")
                    report_lines.append(f"  ÁêÜÁî±: {reason[:100]}")

            if optimization_result.get("retrain_recommended"):
                report_lines.append(f"\n‚ö†Ô∏è <b>ÂÜçÂ≠¶ÁøíÊé®Â•®:</b>")
                report_lines.append(optimization_result.get("retrain_reason", "")[:200])

            extended_suggestions = optimization_result.get("extended_features_to_consider", [])
            if extended_suggestions:
                report_lines.append(f"\nüí° <b>‰ªäÂæåÊ§úË®é„Åô„Åπ„ÅçÁâπÂæ¥Èáè:</b>")
                for feat in extended_suggestions[:3]:
                    report_lines.append(f"‚Ä¢ {feat}")

            await self.executor._send_telegram("\n".join(report_lines))
            logger.info(f"Feature optimization completed: {len(executed_actions)} executed, {len(proposed_actions)} proposed")

        except Exception as e:
            logger.error(f"Feature optimization failed: {e}")
            await self.executor._send_telegram(
                f"‚ùå ÁâπÂæ¥ÈáèÊúÄÈÅ©Âåñ„Åß„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {e}"
            )

    async def _task_database_maintenance(self) -> None:
        """
        Weekly database maintenance task.
        Cleans up old records and optimizes database.
        """
        logger.info("Running database maintenance")

        try:
            # Get stats before cleanup
            stats_before = self.memory.get_database_stats()

            # Clean up old records (3 years default, 5 years for param_history)
            deleted = self.memory.cleanup_old_records()

            # Vacuum to reclaim space
            self.memory.vacuum()

            # Get stats after cleanup
            stats_after = self.memory.get_database_stats()

            # Calculate savings
            size_saved = stats_before["file_size_mb"] - stats_after["file_size_mb"]
            total_deleted = sum(deleted.values())

            # Send report
            lines = [
                "üóÑÔ∏è „Éá„Éº„Çø„Éô„Éº„Çπ„É°„É≥„ÉÜ„Éä„É≥„ÇπÂÆå‰∫Ü",
                "",
                f"ÂâäÈô§„É¨„Ç≥„Éº„ÉâÊï∞: {total_deleted}‰ª∂",
            ]

            if deleted:
                for table, count in deleted.items():
                    if count > 0:
                        lines.append(f"  - {table}: {count}‰ª∂")

            lines.extend([
                "",
                f"DB „Çµ„Ç§„Ç∫: {stats_before['file_size_mb']:.2f}MB ‚Üí {stats_after['file_size_mb']:.2f}MB",
                f"Ëß£ÊîæÂÆπÈáè: {size_saved:.2f}MB",
                "",
                "ÁèæÂú®„ÅÆ„É¨„Ç≥„Éº„ÉâÊï∞:",
            ])

            for table, count in stats_after["tables"].items():
                lines.append(f"  - {table}: {count}‰ª∂")

            await self.executor._send_telegram("\n".join(lines))
            logger.info(f"Database maintenance completed: {total_deleted} records deleted")

        except Exception as e:
            logger.error(f"Database maintenance failed: {e}")
            await self.executor._send_telegram(
                f"‚ùå DB„É°„É≥„ÉÜ„Éä„É≥„Çπ„Åß„Ç®„É©„Éº: {e}"
            )

    async def _task_memory_validation(self) -> None:
        """
        Weekly long-term memory validation task.
        Validates insights and rules against recent data.
        Deprecates items that are no longer valid.
        """
        logger.info("Running long-term memory validation")

        try:
            # 1. Run automatic validation (age-based deprecation)
            validation_results = self.long_term_memory.run_validation()

            # 2. Get items needing LLM validation
            items_to_validate = []
            for insight in self.long_term_memory.get_active_insights():
                items_to_validate.append({
                    "type": "insight",
                    "id": insight.id,
                    "category": insight.category,
                    "title": insight.title,
                    "content": insight.content,
                    "success_rate": f"{insight.success_rate:.0%}",
                    "verification_count": insight.verification_count,
                })
            for rule in self.long_term_memory.get_active_rules():
                items_to_validate.append({
                    "type": "rule",
                    "id": rule.id,
                    "name": rule.name,
                    "content": rule.content,
                    "success_rate": f"{rule.success_rate:.0%}",
                    "application_count": rule.application_count,
                })

            # 3. Get recent performance for context
            signal_stats = self.memory.get_signal_accuracy_stats(days=7)
            intervention_stats = self.memory.get_intervention_stats(days=7)

            recent_performance = {
                "signal_accuracy_7d": signal_stats,
                "intervention_stats_7d": intervention_stats,
            }

            # 4. Validate with Claude (if there are items to validate)
            llm_validations = {}
            if items_to_validate:
                llm_results = await self.claude.validate_memory_items(
                    items_to_validate=items_to_validate[:10],  # Limit to 10
                    recent_performance=recent_performance,
                )

                # Process LLM validation results
                for validation in llm_results.get("validations", []):
                    item_id = validation.get("id")
                    item_type = validation.get("type")
                    success = validation.get("success")
                    recommendation = validation.get("recommendation")

                    if item_type == "insight" and item_id:
                        if success is not None:
                            self.long_term_memory.verify_insight(
                                item_id,
                                success=success,
                                notes=validation.get("notes", ""),
                            )
                        llm_validations[f"insight:{item_id}"] = recommendation
                    elif item_type == "rule" and item_id:
                        if success is not None:
                            self.long_term_memory.apply_rule(
                                item_id,
                                success=success,
                                context=validation.get("notes", ""),
                            )
                        llm_validations[f"rule:{item_id}"] = recommendation

            # 5. Build and send report
            stats = self.long_term_memory.get_stats()
            deprecate_count = sum(
                1 for rec in llm_validations.values()
                if rec == "deprecate"
            )

            lines = [
                "üß† Èï∑ÊúüË®òÊÜ∂„ÅÆÊ§úË®ºÂÆå‰∫Ü",
                "",
                "**Ë®òÊÜ∂„ÅÆÁä∂ÊÖã:**",
                f"- Ê¥ûÂØü: {stats['insights']['active']}‰ª∂ („É¨„Éì„É•„Éº‰∏≠: {stats['insights']['under_review']}‰ª∂)",
                f"- „É´„Éº„É´: {stats['rules']['active']}‰ª∂ („É¨„Éì„É•„Éº‰∏≠: {stats['rules']['under_review']}‰ª∂)",
                f"- „Ç§„Éô„É≥„Éà: {stats['events']['total']}‰ª∂",
            ]

            if validation_results["items_needing_attention"]:
                lines.append("")
                lines.append("**Ë¶ÅÊ≥®ÊÑèÈ†ÖÁõÆ:**")
                for item in validation_results["items_needing_attention"][:5]:
                    item_name = item.get("title") or item.get("name", "‰∏çÊòé")
                    lines.append(f"- {item['type']}: {item_name} ({item['reason']})")

            if deprecate_count > 0:
                lines.append("")
                lines.append(f"‚ö†Ô∏è Ê∑òÊ±∞Êé®Â•®: {deprecate_count}‰ª∂")
                lines.append("ÔºàÈÅéÂ≠¶Áøí„ÇíÈò≤„Åê„Åü„ÇÅ„ÄÅÊúâÂäπÊÄß„ÅÆ‰Ωé„ÅÑÈ†ÖÁõÆ„ÇíËá™ÂãïÁöÑ„Å´ÁÑ°ÂäπÂåñ„Åó„Åæ„Åó„ÅüÔºâ")

            await self.executor._send_telegram("\n".join(lines))
            logger.info(
                f"Memory validation completed: "
                f"{stats['insights']['active']} insights, {stats['rules']['active']} rules active"
            )

            # Save weekly reflection
            now = now_jst()
            week_start = now - timedelta(days=7)

            # Build memory updates list
            memory_updates = []
            if validation_results["insights_reviewed"] > 0:
                memory_updates.append(f"Ê¥ûÂØü{validation_results['insights_reviewed']}‰ª∂„Çí„É¨„Éì„É•„Éº‰∏≠„Å´ÁßªË°å")
            if validation_results["insights_deprecated"] > 0:
                memory_updates.append(f"Ê¥ûÂØü{validation_results['insights_deprecated']}‰ª∂„ÇíÊ∑òÊ±∞")
            if validation_results["rules_reviewed"] > 0:
                memory_updates.append(f"„É´„Éº„É´{validation_results['rules_reviewed']}‰ª∂„Çí„É¨„Éì„É•„Éº‰∏≠„Å´ÁßªË°å")
            if validation_results["rules_deprecated"] > 0:
                memory_updates.append(f"„É´„Éº„É´{validation_results['rules_deprecated']}‰ª∂„ÇíÊ∑òÊ±∞")
            if deprecate_count > 0:
                memory_updates.append(f"LLMÊ§úË®º„Åß{deprecate_count}‰ª∂„Å´Ê∑òÊ±∞„ÇíÊé®Â•®")

            # Determine good things and improvements
            good_things = []
            improvements_needed = []

            if signal_stats.get("accuracy", 0) >= 0.6:
                good_things.append(f"„Ç∑„Ç∞„Éä„É´Á≤æÂ∫¶„ÅåËâØÂ•Ω ({signal_stats['accuracy']:.0%})")
            else:
                improvements_needed.append(f"„Ç∑„Ç∞„Éä„É´Á≤æÂ∫¶„ÅÆÊîπÂñÑ„ÅåÂøÖË¶Å ({signal_stats['accuracy']:.0%})")

            if stats['insights']['active'] > 0:
                good_things.append(f"{stats['insights']['active']}‰ª∂„ÅÆÊúâÂäπ„Å™Ê¥ûÂØü„ÇíÁ∂≠ÊåÅ")
            if stats['rules']['active'] > 0:
                good_things.append(f"{stats['rules']['active']}‰ª∂„ÅÆÊúâÂäπ„Å™„É´„Éº„É´„ÇíÁ∂≠ÊåÅ")

            if deprecate_count > 0:
                improvements_needed.append("ÈÅéÂ≠¶Áøí„ÅÆÂÖÜÂÄô„ÅÇ„Çä„ÄÅ‰∏ÄÈÉ®„ÅÆË®òÊÜ∂„ÇíÊ∑òÊ±∞")

            self.long_term_memory.add_weekly_reflection(
                start_date=week_start,
                end_date=now,
                performance_summary={
                    "signal_accuracy": signal_stats.get("accuracy", 0),
                    "intervention_success": intervention_stats.get("total", 0) > 0,
                    "major_mistakes": intervention_stats.get("obvious_misses", 0),
                },
                good_things=good_things if good_things else ["Áâπ„Å´„Å™„Åó"],
                improvements_needed=improvements_needed if improvements_needed else ["Áâπ„Å´„Å™„Åó"],
                focus_points=["Á∂ôÁ∂öÁöÑ„Å™Ë®òÊÜ∂„ÅÆÊ§úË®º", "ÈÅéÂ≠¶Áøí„ÅÆÈò≤Ê≠¢"],
                memory_updates=memory_updates if memory_updates else ["Â§âÊõ¥„Å™„Åó"],
            )
            logger.info("Weekly reflection saved to long-term memory")

        except Exception as e:
            logger.error(f"Memory validation failed: {e}")
            await self.executor._send_telegram(
                f"‚ùå Ë®òÊÜ∂Ê§úË®º„Åß„Ç®„É©„Éº: {e}"
            )

    # ==================== Trigger Handling ====================

    async def _check_triggers(self) -> None:
        """Check for and process manual triggers from API/Telegram."""
        trigger_path = Path("data/agent_triggers.json")
        if not trigger_path.exists():
            return

        try:
            with open(trigger_path) as f:
                triggers = json.load(f)

            if not triggers:
                return

            # Process pending triggers
            updated = False
            for trigger_name, trigger_data in list(triggers.items()):
                if trigger_data.get("status") != "pending":
                    continue

                logger.info(f"Processing trigger: {trigger_name}")
                source = trigger_data.get("source", "api")

                try:
                    if trigger_name == "daily_review":
                        await self._task_daily_review()
                        triggers[trigger_name]["status"] = "completed"
                        triggers[trigger_name]["completed_at"] = now_jst().isoformat()

                    elif trigger_name == "signal_verification":
                        await self._task_signal_verification()
                        triggers[trigger_name]["status"] = "completed"
                        triggers[trigger_name]["completed_at"] = now_jst().isoformat()

                    elif trigger_name == "emergency_analysis":
                        context = trigger_data.get("context", "")
                        await self._emergency_analysis(context)
                        triggers[trigger_name]["status"] = "completed"
                        triggers[trigger_name]["completed_at"] = now_jst().isoformat()

                    elif trigger_name == "feature_optimization":
                        await self._task_feature_optimization()
                        triggers[trigger_name]["status"] = "completed"
                        triggers[trigger_name]["completed_at"] = now_jst().isoformat()

                    updated = True
                    logger.info(f"Trigger {trigger_name} completed (source: {source})")

                except Exception as e:
                    logger.error(f"Error processing trigger {trigger_name}: {e}")
                    triggers[trigger_name]["status"] = "failed"
                    triggers[trigger_name]["error"] = str(e)
                    updated = True

            # Save updated triggers
            if updated:
                with open(trigger_path, "w") as f:
                    json.dump(triggers, f, indent=2)

        except Exception as e:
            logger.error(f"Error checking triggers: {e}")

    async def _update_status_file(self) -> None:
        """Update agent status file for API/Telegram to read."""
        try:
            status_path = Path("data/agent_status.json")
            status_path.parent.mkdir(parents=True, exist_ok=True)

            # Get recent actions from memory
            recent_decisions = self.memory.get_decision_history_summary(limit=5)

            status = {
                "status": "running" if self._running else "stopped",
                "last_check": now_jst().isoformat(),
                "decisions_today": self._count_decisions_today(),
                "consecutive_errors": self._consecutive_errors,
                "recent_actions": self._format_recent_actions(recent_decisions),
            }

            with open(status_path, "w") as f:
                json.dump(status, f, indent=2)

        except Exception as e:
            logger.error(f"Error updating status file: {e}")

    def _count_decisions_today(self) -> int:
        """Count decisions made today."""
        try:
            # Simple count from memory
            return self.memory.get_decision_count_today()
        except Exception:
            return 0

    def _format_recent_actions(self, decisions: str) -> list[dict]:
        """Format recent decisions as action list."""
        # This is a simplified version - could parse the summary string
        actions = []
        if self._last_decision_time:
            actions.append({
                "type": "decision",
                "summary": "ÊúÄÊñ∞„ÅÆÂà§Êñ≠",
                "time": self._last_decision_time.isoformat(),
            })
        return actions

    async def _emergency_analysis(self, context: str = "") -> None:
        """
        Run emergency analysis.

        This is triggered manually when immediate analysis is needed.
        """
        logger.warning(f"Running emergency analysis: {context}")

        # Notify start
        await self.executor._send_telegram(
            f"üö® <b>Á∑äÊÄ•ÂàÜÊûêÈñãÂßã</b>\n"
            f"„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà: {context or '„Å™„Åó'}\n"
            f"ÊôÇÂàª: {now_jst().strftime('%H:%M:%S')}"
        )

        # Gather comprehensive context
        full_context = await self.perception.get_context()

        # Build emergency prompt
        emergency_prompt = f"""
Á∑äÊÄ•ÂàÜÊûê„É™„ÇØ„Ç®„Çπ„Éà

„É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà: {context or 'ÊåáÂÆö„Å™„Åó'}

ÁèæÂú®„ÅÆÁä∂Ê≥Å:
{full_context.to_prompt()}

„Åì„ÅÆÁä∂Ê≥Å„ÇíÂç≥Â∫ß„Å´ÂàÜÊûê„Åó„ÄÅÂøÖË¶Å„Å™„Ç¢„ÇØ„Ç∑„Éß„É≥„ÇíÊ±∫ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Á∑äÊÄ•Â∫¶„ÅÆÈ´ò„ÅÑÂïèÈ°å„Åå„ÅÇ„Çå„Å∞„ÄÅÈÅ©Âàá„Å™ÂØæÂøú„ÇíÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
"""

        # Get Claude's analysis
        memory_summary = self.memory.get_decision_history_summary(limit=5)

        decision = await self.claude.analyze_and_decide(
            context_prompt=emergency_prompt,
            memory_summary=memory_summary,
        )

        # Build response
        response_lines = [
            f"üîç <b>Á∑äÊÄ•ÂàÜÊûêÂÆå‰∫Ü</b>",
            f"",
            f"<b>ÂàÜÊûêÁµêÊûú:</b>",
            f"{decision.reasoning[:1000]}",
            f"",
            f"<b>Êé®Â•®„Ç¢„ÇØ„Ç∑„Éß„É≥:</b> {len(decision.actions)}‰ª∂",
        ]

        for action in decision.actions[:5]:
            response_lines.append(f"‚Ä¢ {action.type}: {action.description}")

        if decision.actions:
            # Execute high-priority actions
            results = await self.executor.execute_actions(decision.actions)
            response_lines.append(f"\n<b>ÂÆüË°åÁµêÊûú:</b> {'ÊàêÂäü' if results.overall_success else '‰∏ÄÈÉ®Â§±Êïó'}")

        await self.executor._send_telegram("\n".join(response_lines))
        logger.info("Emergency analysis completed")

    # ==================== Public Methods ====================

    async def force_daily_review(self) -> None:
        """Manually trigger daily review."""
        await self._task_daily_review()

    async def force_signal_verification(self) -> None:
        """Manually trigger signal verification."""
        await self._task_signal_verification()

    async def force_emergency_analysis(self, context: str = "") -> None:
        """Manually trigger emergency analysis."""
        await self._emergency_analysis(context)

    def get_status(self) -> dict:
        """Get agent status."""
        return {
            "running": self._running,
            "last_decision_time": self._last_decision_time.isoformat() if self._last_decision_time else None,
            "consecutive_errors": self._consecutive_errors,
            "scheduled_tasks": self.scheduler.get_all_status(),
            "memory_stats": {
                "signal_accuracy_7d": self.memory.get_signal_accuracy_stats(days=7),
                "decision_patterns": self.memory.get_decision_patterns(),
            },
        }
